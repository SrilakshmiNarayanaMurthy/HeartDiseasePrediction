# -*- coding: utf-8 -*-
"""Heart_Disease_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JhsoaOCqF-2KVFTVHQtn8mAjO8GZ_v2z
"""

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
#import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import LinearSVC
from sklearn import svm
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import log_loss,roc_auc_score,precision_score,f1_score,recall_score,roc_curve,auc
from sklearn.metrics import confusion_matrix,accuracy_score,matthews_corrcoef

df = pd.read_csv('Heart_Disease_Data.csv', na_values='?')
df.head()

df.shape

df.info()

df['pred_attribute'].value_counts()

df.describe(include='all')

df.isnull().sum()

df.groupby('pred_attribute')['pred_attribute'].count()

df['pred_attribute'] = df['pred_attribute'].astype(bool).astype(int)

plt.hist(df['age'], color='blue', edgecolor='black', bins=24)
plt.xlabel('Age of the patient')
plt.ylabel('Frequency')
plt.title('Age distribution')
plt.show()

df['age'] = (df['age'] - 54.43)/48

men = df['sex'].sum() # '1' indicates male and '0' indicates female
print('There are '+ str(men) + ' males and '+ str(df.shape[0] - men) + ' females in the dataset')

df.groupby('cp')['cp'].count()

plt.hist(df['trestbps'], color='blue', edgecolor='black', bins=int(106/3))
plt.xlabel('Blood pressure')
plt.ylabel('Frequency')
plt.title('Resting Blood pressure distribution')
plt.show()

df['trestbps'] = (df['trestbps']-131.69)/106

plt.hist(df['chol'], color='blue', edgecolor='black', bins=73)
plt.xlabel('Serum cholestrol(mg/dl)')
plt.ylabel('Frequency')
plt.title('Serum cholestrol distribution')
plt.show()

df['chol'] = (df['chol']-240)/250

diabetic = df['fbs'].sum() # '1' indicates yes and '0' indicates no about resting blood sugar
print('There are '+ str(diabetic) + ' patients who had resting blood sugar > 120 mg/dl out of the '+ str(df.shape[0]) + ' patients')

df.groupby('restecg')['restecg'].count()

plt.hist(df['thalach'], color='blue', edgecolor='black', bins=20)
plt.xlabel('Max heart rate achieved')
plt.ylabel('Frequency')
plt.title('Max heart rate distribution')
plt.show()

df['thalach'] = (df['thalach'] - 149.6)/131

eig = df['exang'].sum() # '1' indicates yes and '0' indicates no about exercise induced angina
print('There are '+ str(eig) + ' patients who had exercise induced angina out of the '+ str(df.shape[0]) + ' patients')

plt.hist(df['oldpeak'], color='blue', edgecolor='black', bins=20)
plt.xlabel('ST depression induced by exercise relative to rest')
plt.ylabel('Frequency')
plt.title('Old peak distribution')
plt.show()

df['oldpeak'] = (df['oldpeak']-1.04)/6.2

df.groupby('slop')['slop'].count()

df.groupby('ca')['ca'].count()

df['ca'] = df['ca'].fillna(df['ca'].mode()[0])

df.groupby('ca')['ca'].count()

df.groupby('thal')['thal'].count()

df['thal'] = df['thal'].fillna(df['thal'].mode()[0])

df.groupby('thal')['thal'].count()

df.isnull().sum()

"""# # ML models"""

y = df['pred_attribute']
df.drop('pred_attribute', axis=1, inplace=True)
x = df

rstate = 100
x_train, x_test, y_train, y_test = train_test_split(x,y,random_state=rstate)

print(x.shape, x_train.shape, x_test.shape)

model = LogisticRegression(solver='lbfgs', multi_class='multinomial',C=1e-2,max_iter=500,tol=20)
model.fit(x_train, y_train)

predictions_lr = model.predict(x_test)
accuracy_score(predictions_lr, y_test) * 100

classifier = GaussianNB()
classifier.fit(x_train,y_train)

predictions_gnb = model.predict(x_test)
accuracy_score(predictions_gnb, y_test) * 100

model = LinearSVC(max_iter=2000, random_state=rstate)
model.fit(x_train,y_train)

predictions_lsvc = model.predict(x_test)
accuracy_score(predictions_lsvc, y_test) * 100

model = tree.DecisionTreeClassifier(random_state=rstate)
model.fit(x_train,y_train)

predictions_dt = model.predict(x_test)
accuracy_score(predictions_dt, y_test) * 100

model_rf = RandomForestClassifier(random_state=rstate,max_leaf_nodes=16)
model_rf.fit(x_train, y_train)

predictions_rf = model.predict(x_test)
accuracy_score(predictions_rf, y_test) * 100

model.feature_importances_

model = KNeighborsClassifier()
model.fit(x_train,y_train)

predictions_knn = model.predict(x_test)
accuracy_score(predictions_knn, y_test) * 100

model = svm.SVC(C=2, random_state=rstate,tol=1)
model.fit(x_train,y_train)

predictions_svm = model.predict(x_test)
accuracy_score(predictions_svm, y_test) * 100

feat_importances = pd.Series(model_rf.feature_importances_, index=x_train.columns)
feat_importances.nlargest(20).plot(kind='barh')

CM=confusion_matrix(y_test,predictions_rf)
sns.heatmap(CM, annot=True)

TN = CM[0][0]
FN = CM[1][0]
TP = CM[1][1]
FP = CM[0][1]
specificity = TN/(TN+FP)
loss_log = log_loss(y_test, predictions_rf)
acc= accuracy_score(y_test, predictions_rf)
roc=roc_auc_score(y_test, predictions_rf)
prec = precision_score(y_test, predictions_rf)
rec = recall_score(y_test, predictions_rf)
f1 = f1_score(y_test, predictions_rf)

mathew = matthews_corrcoef(y_test, predictions_rf)
model_results =pd.DataFrame([['Random Forest',acc, prec,rec,specificity, f1,roc, loss_log,mathew]],
               columns = ['Model', 'Accuracy','Precision', 'Sensitivity','Specificity', 'F1 Score','ROC','Log_Loss','mathew_corrcoef'])

model_results

data = {        'Logistic Regretion': predictions_lr, 
                'Gaussian NB': predictions_gnb, 
                'Linear SVC': predictions_lsvc,
                'Decision tree': predictions_dt, 
                'KNN': predictions_knn, 
                'SVM':predictions_svm
                }

models = pd.DataFrame(data) 
 
for column in models:
    CM=confusion_matrix(y_test,models[column])
    
    TN = CM[0][0]
    FN = CM[1][0]
    TP = CM[1][1]
    FP = CM[0][1]
    specificity = TN/(TN+FP)
    loss_log = log_loss(y_test, models[column])
    acc= accuracy_score(y_test, models[column])
    roc=roc_auc_score(y_test, models[column])
    prec = precision_score(y_test, models[column])
    rec = recall_score(y_test, models[column])
    f1 = f1_score(y_test, models[column])
    
    mathew = matthews_corrcoef(y_test, models[column])
    results =pd.DataFrame([[column,acc, prec,rec,specificity, f1,roc, loss_log,mathew]],
               columns = ['Model', 'Accuracy','Precision', 'Sensitivity','Specificity', 'F1 Score','ROC','Log_Loss','mathew_corrcoef'])
    model_results = model_results.append(results, ignore_index = True)

model_results

import pickle

filename='trained_model.sav'
pickle.dump(classifier, open(filename, 'wb'))

